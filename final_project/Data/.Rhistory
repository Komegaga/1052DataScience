(prob$prediction == "Yes")[f]
for(f in 1:1470){
if( (prob$prediction == "Yes")[f] ){
tmpiris$pred.scores <- prob$Yes
}
for(f in 1:1470){
if( (prob$prediction == "Yes")[f] ){
tmpiris$pred.scores <- prob$Yes
}
}
for(f in 1:1470){
print((prob$prediction == "Yes")[f])
}
for(f in 1:1470){
(prob$prediction == "Yes")[f])
}
for(f in 1:1470){
(prob$prediction == "Yes")[f]
}
prob$Yes[f]
for(f in 1:1470){
if( (prob$prediction == "Yes")[f] ){
tmpiris$pred.scores <- prob$Yes[f]
}
}
prob[,2][1]
prob[,2][f]
for (i in 2010:2015){
print(paste("The year is", i))
}
for(f in 1:1470){
print(prob[i,2])
}
for(i in 1:1470){
print(prob[i,2])
}
tail(tmpiris)
(prob$prediction == "Yes")[i]
for(i in 1:1470){
if( (prob$prediction == "Yes")[i] ){
tmpiris$pred.scores <- prob[i,2]
} else {
tmpiris$pred.scores <- prob[i,1]
}
}
(prob$prediction == "Yes")[i]
yes <- prob$Yes
no <- prob$No
result <-
ifelse(prob$prediction=="Yes",prob$Yes,ifelse(prob$prediction=="No",prob$No))
?ifelse
result <-
ifelse(prob$prediction=="Yes",prob$Yes,prob$No)
result <- ifelse(prob$prediction=="Yes",prob$Yes,prob$No)
tmpiris$pred.scores <- result
prob$pred.score <- result
d <- prob[,c(-1:-2)]
View(d)
cM <- table(truth=d$reference, prediction=d$prediction)
sensitivity <- round ( cM[2, 2] / (cM[2, 1] + cM[2, 2]), digits = 2)
specificity <- round ( cM[1, 1] / (cM[1, 1] + cM[1, 2]), digits = 2)
precision <- round ( cM[2, 2] / (cM[1, 2] + cM[2, 2]), digits = 2)
F1 <- round ( 2 *  precision * sensitivity / (precision + sensitivity), digits = 2)
library('ROCR')
eval <- prediction(d$pred.score,d$reference)
AUC <- round(attributes(performance(eval,'auc'))$y.values[[1]], digits = 2)
attributes(performance(eval,'auc'))$y.values[[1]]
performance(eval,'auc')
attributes(performance(eval,'auc'))
?prediction
eval <- prediction(d$prediction,d$reference)
d$prediction
d$reference
as.vector(d$prediction)
eval <- prediction(as.vector(d$prediction),as.vector(d$reference))
eval <- prediction(as.list(d$prediction),as.list(d$reference))
?ROCR
eval <- prediction(as.matrix(d$prediction),as.matrix(d$reference))
eval <- prediction(d$prediction,d$reference)
plot(performance(eval,"tpr","fpr"))
eval <- prediction(d$pred.score,d$reference)
plot(performance(eval,"tpr","fpr"))
prob$pred.score <- prob$Yes
View(prob)
d <- prob[,c(-1:-2)]
eval <- prediction(d$pred.score,d$reference)
plot(performance(eval,"tpr","fpr"))
AUC <- round(attributes(performance(eval,'auc'))$y.values[[1]], digits = 2)
testset+i <- subset(data, id %in% c(i))
for (i in 1:k){
# remove rows with id i from dataframe to create training set
# select rows with id i to create test set
dTrainAll <- subset(data, id %in% list[-i])
testset <- subset(data, id %in% c(i))
useForCal <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=1/ (k-1))>0 	# Note: 14
dCal <- subset(dTrainAll,useForCal)
dTrain <- subset(dTrainAll,!useForCal)
# run a random forest model
mymodel <- randomForest(Attrition ~ ., data = dTrain, ntree = 100,importance=TRUE
,proximity=TRUE)
testErro <- myPred(mymodel,testset)
calError <- myPred(mymodel,dCal)
table.model <- mymodel$confusion
trainErro <- sum(diag(table.model)/sum(table.model))
trainErros <- c(trainErros,trainErro)
calErrors <- c(calErrors,calError)
testErrors <- c(testErrors,testErro)
files <- c(files,dTrainAll)
progress.bar$step()
}
files <- c()
for (i in 1:k){
# remove rows with id i from dataframe to create training set
# select rows with id i to create test set
dTrainAll <- subset(data, id %in% list[-i])
testset <- subset(data, id %in% c(i))
useForCal <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=1/ (k-1))>0 	# Note: 14
dCal <- subset(dTrainAll,useForCal)
dTrain <- subset(dTrainAll,!useForCal)
# run a random forest model
mymodel <- randomForest(Attrition ~ ., data = dTrain, ntree = 100,importance=TRUE
,proximity=TRUE)
testErro <- myPred(mymodel,testset)
calError <- myPred(mymodel,dCal)
table.model <- mymodel$confusion
trainErro <- sum(diag(table.model)/sum(table.model))
trainErros <- c(trainErros,trainErro)
calErrors <- c(calErrors,calError)
testErrors <- c(testErrors,testErro)
files <- c(files,dTrainAll)
progress.bar$step()
}
files
files[1]
write.csv(dTrainAll, file = "dTrainAll"+ i + ".csv", row.names = F, quote = F)
write.csv(dTrainAll, file =paste0("dTrainAll"+ i + ".csv") , row.names = F, quote = F)
write.csv(dTrainAll,paste0("dTrainAll"+ i + ".csv") , row.names = F, quote = F)
write.csv(dTrainAll,paste0("dTrainAll", i,".csv") , row.names = F, quote = F)
write.csv(dTrainAll,file = "dTrainAll", i , ".csv" , row.names = F, quote = F)
for (i in 1:k){
# remove rows with id i from dataframe to create training set
# select rows with id i to create test set
dTrainAll <- subset(data, id %in% list[-i])
testset <- subset(data, id %in% c(i))
useForCal <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=1/ (k-1))>0 	# Note: 14
dCal <- subset(dTrainAll,useForCal)
dTrain <- subset(dTrainAll,!useForCal)
# run a random forest model
mymodel <- randomForest(Attrition ~ ., data = dTrain, ntree = 100,importance=TRUE
,proximity=TRUE)
testErro <- myPred(mymodel,testset)
calError <- myPred(mymodel,dCal)
table.model <- mymodel$confusion
trainErro <- sum(diag(table.model)/sum(table.model))
trainErros <- c(trainErros,trainErro)
calErrors <- c(calErrors,calError)
testErrors <- c(testErrors,testErro)
write.csv(dTrainAll,paste0("dTrainAll_", i ,".csv") , row.names = F, quote = F)
progress.bar$step()
}
for (i in 1:k) {
file <- paste0("dTrainAll_", i ,".csv")
files <- c(files,file)
}
files
files <- 0
for (i in 1:k) {
file <- paste0("dTrainAll_", i ,".csv")
files <- c(files,file)
}
files <- c()
for (i in 1:k) {
file <- paste0("dTrainAll_", i ,".csv")
files <- c(files,file)
}
View(d)
data<-read.table(file, header=T,sep=",")
prob <-as.data.frame(predict(mymodel,data[,-2],type="prob"))
prob_res <-as.data.frame(predict(mymodel,data[,-2],type="response"))
result <- ifelse(prob$prediction=="Yes",prob$Yes,prob$No)
prob$pred.score <- prob$Yes
prob$reference <- data[,2]
prob$prediction <- prob_res[,1]
d <- prob[,c(-1:-2)]
View(d)
cM <- table(truth=d$reference, prediction=d$prediction)
sensitivity <- round ( cM[2, 2] / (cM[2, 1] + cM[2, 2]), digits = 2)
specificity <- round ( cM[1, 1] / (cM[1, 1] + cM[1, 2]), digits = 2)
precision <- round ( cM[2, 2] / (cM[1, 2] + cM[2, 2]), digits = 2)
F1 <- round ( 2 *  precision * sensitivity / (precision + sensitivity), digits = 2)
library('ROCR')
eval <- prediction(d$pred.score,d$reference)
plot(performance(eval,"tpr","fpr"))
AUC <- round(attributes(performance(eval,'auc'))$y.values[[1]], digits = 2)
for(file in files)
{
name<-gsub(".csv", "", basename(file))
data<-read.table(file, header=T,sep=",")
prob <-as.data.frame(predict(mymodel,data[,-2],type="prob"))
prob_res <-as.data.frame(predict(mymodel,data[,-2],type="response"))
result <- ifelse(prob$prediction=="Yes",prob$Yes,prob$No)
prob$pred.score <- prob$Yes
prob$reference <- data[,2]
prob$prediction <- prob_res[,1]
d <- prob[,c(-1:-2)]
cM <- table(truth=d$reference, prediction=d$prediction)
sensitivity <- round ( cM[2, 2] / (cM[2, 1] + cM[2, 2]), digits = 2)
specificity <- round ( cM[1, 1] / (cM[1, 1] + cM[1, 2]), digits = 2)
precision <- round ( cM[2, 2] / (cM[1, 2] + cM[2, 2]), digits = 2)
F1 <- round ( 2 *  precision * sensitivity / (precision + sensitivity), digits = 2)
# AUC evaluation
library('ROCR')
eval <- prediction(d$pred.score,d$reference)
AUC <- round(attributes(performance(eval,'auc'))$y.values[[1]], digits = 2)
sens<-c(sens, sensitivity)
spes<-c(spes, specificity)
F1s<-c(F1s, F1)
AUCs<-c(AUCs, AUC)
names<-c(names,name)
}
for(file in files){
name<-gsub(".csv", "", basename(file))
data<-read.table(file, header=T,sep=",")
prob <-as.data.frame(predict(mymodel,data[,-2],type="prob"))
prob_res <-as.data.frame(predict(mymodel,data[,-2],type="response"))
result <- ifelse(prob$prediction=="Yes",prob$Yes,prob$No)
prob$pred.score <- prob$Yes
prob$reference <- data[,2]
prob$prediction <- prob_res[,1]
d <- prob[,c(-1:-2)]
cM <- table(truth=d$reference, prediction=d$prediction)
sensitivity <- round ( cM[2, 2] / (cM[2, 1] + cM[2, 2]), digits = 2)
specificity <- round ( cM[1, 1] / (cM[1, 1] + cM[1, 2]), digits = 2)
precision <- round ( cM[2, 2] / (cM[1, 2] + cM[2, 2]), digits = 2)
F1 <- round ( 2 *  precision * sensitivity / (precision + sensitivity), digits = 2)
# AUC evaluation
library('ROCR')
eval <- prediction(d$pred.score,d$reference)
AUC <- round(attributes(performance(eval,'auc'))$y.values[[1]], digits = 2)
sens<-c(sens, sensitivity)
spes<-c(spes, specificity)
F1s<-c(F1s, F1)
AUCs<-c(AUCs, AUC)
names<-c(names,name)
}
names<-c()
sens<-c()
spes<-c()
F1s<-c()
AUCs<-c()
for(file in files){
name<-gsub(".csv", "", basename(file))
data<-read.table(file, header=T,sep=",")
prob <-as.data.frame(predict(mymodel,data[,-2],type="prob"))
prob_res <-as.data.frame(predict(mymodel,data[,-2],type="response"))
result <- ifelse(prob$prediction=="Yes",prob$Yes,prob$No)
prob$pred.score <- prob$Yes
prob$reference <- data[,2]
prob$prediction <- prob_res[,1]
d <- prob[,c(-1:-2)]
cM <- table(truth=d$reference, prediction=d$prediction)
sensitivity <- round ( cM[2, 2] / (cM[2, 1] + cM[2, 2]), digits = 2)
specificity <- round ( cM[1, 1] / (cM[1, 1] + cM[1, 2]), digits = 2)
precision <- round ( cM[2, 2] / (cM[1, 2] + cM[2, 2]), digits = 2)
F1 <- round ( 2 *  precision * sensitivity / (precision + sensitivity), digits = 2)
# AUC evaluation
library('ROCR')
eval <- prediction(d$pred.score,d$reference)
AUC <- round(attributes(performance(eval,'auc'))$y.values[[1]], digits = 2)
sens<-c(sens, sensitivity)
spes<-c(spes, specificity)
F1s<-c(F1s, F1)
AUCs<-c(AUCs, AUC)
names<-c(names,name)
}
View(data_50000)
system("Rscript final_105753032.R -fold 5 -out performance.csv")
system("Rscript final_105753032.R -fold 5 -out performance.csv")
system("Rscript final_105753032.R -fold 10 -out performance.csv")
data <- read.csv("WA_Fn-UseC_-HR-Employee-Attrition.csv",header=T,na.strings='NULL')
library(plyr)
library(randomForest)
k =  10 #Folds
k = as.integer(query_n)
set.seed(9487)
data$id <- sample(1:k, nrow(data), replace = TRUE)
list <- 1:k
trainErros <- c()
calErrors <- c()
testErrors <- c()
progress.bar <- create_progress_bar("text")
progress.bar$init(k)
myPred <- function(mymodel,dataset){
prediction <- data.frame()
testsetCopy <- data.frame()
temp <- as.data.frame(predict(mymodel, dataset[,-2]))
prediction <- rbind(prediction, temp)
testsetCopy <- rbind(testsetCopy, as.data.frame(dataset[,2]))
result <- cbind(prediction, testsetCopy[, 1])
names(result) <- c("Predicted", "Actual")
for (i in 1:length(result$Actual)){
if (result$Actual[i] == result$Predicted[i]) {
result$Difference[i] <- 1
} else {
result$Difference[i] <- 0
}
}
testErro <- count(result$Difference)[2,2]/sum(count(result$Difference)$freq)
testErro
}
for (i in 1:k){
# remove rows with id i from dataframe to create training set
# select rows with id i to create test set
dTrainAll <- subset(data, id %in% list[-i])
testset <- subset(data, id %in% c(i))
useForCal <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=1/ (k-1))>0 	# Note: 14
dCal <- subset(dTrainAll,useForCal)
dTrain <- subset(dTrainAll,!useForCal)
# run a random forest model
mymodel <- randomForest(Attrition ~ ., data = dTrain, ntree = 100,importance=TRUE
,proximity=TRUE)
testErro <- myPred(mymodel,testset)
calError <- myPred(mymodel,dCal)
table.model <- mymodel$confusion
trainErro <- sum(diag(table.model)/sum(table.model))
trainErros <- c(trainErros,trainErro)
calErrors <- c(calErrors,calError)
testErrors <- c(testErrors,testErro)
write.csv(dTrainAll,paste0("dTrainAll_", i ,".csv") , row.names = F, quote = F)
progress.bar$step()
}
set <- c("trainning","calibration","test")
accuracy <- c(round(mean(trainErros),digits = 2),round(mean(calErrors),digits = 2),round(mean(testErrors), digits = 2))
out_data<-data.frame("set"=set, "accuracy"=accuracy,stringsAsFactors = F)
files <- c()
for (i in 1:k) {
file <- paste0("dTrainAll_", i ,".csv")
files <- c(files,file)
}
names<-c()
sens<-c()
spes<-c()
F1s<-c()
AUCs<-c()
name<-gsub(".csv", "", basename(file))
data<-read.table(file, header=T,sep=",")
prob <-as.data.frame(predict(mymodel,data[,-2],type="prob"))
prob_res <-as.data.frame(predict(mymodel,data[,-2],type="response"))
result <- ifelse(prob$prediction=="Yes",prob$Yes,prob$No)
prob$pred.score <- prob$Yes
prob$reference <- data[,2]
prob$prediction <- prob_res[,1]
d <- prob[,c(-1:-2)]
cM <- table(truth=d$reference, prediction=d$prediction)
cM
table(dTrainAll$Attrition)
View(dTrainAll)
out_data <-data.frame("method"=files, "sensitivity"=sens, "specificity"=spes, "F1"=F1s, "AUC"=AUCs, stringsAsFactors = F)
names<-c()
sens<-c()
spes<-c()
F1s<-c()
AUCs<-c()
for(file in files){
name<-gsub(".csv", "", basename(file))
data<-read.table(file, header=T,sep=",")
prob <-as.data.frame(predict(mymodel,data[,-2],type="prob"))
prob_res <-as.data.frame(predict(mymodel,data[,-2],type="response"))
result <- ifelse(prob$prediction=="Yes",prob$Yes,prob$No)
prob$pred.score <- prob$Yes
prob$reference <- data[,2]
prob$prediction <- prob_res[,1]
d <- prob[,c(-1:-2)]
cM <- table(truth=d$reference, prediction=d$prediction)
sensitivity <- round ( cM[2, 2] / (cM[2, 1] + cM[2, 2]), digits = 2)
specificity <- round ( cM[1, 1] / (cM[1, 1] + cM[1, 2]), digits = 2)
precision <- round ( cM[2, 2] / (cM[1, 2] + cM[2, 2]), digits = 2)
F1 <- round ( 2 *  precision * sensitivity / (precision + sensitivity), digits = 2)
# AUC evaluation
library('ROCR')
eval <- prediction(d$pred.score,d$reference)
AUC <- round(attributes(performance(eval,'auc'))$y.values[[1]], digits = 2)
sens<-c(sens, sensitivity)
spes<-c(spes, specificity)
F1s<-c(F1s, F1)
AUCs<-c(AUCs, AUC)
names<-c(names,name)
}
out_data <-data.frame("method"=files, "sensitivity"=sens, "specificity"=spes, "F1"=F1s, "AUC"=AUCs, stringsAsFactors = F)
B <- out_data[which.max(out_data$F1),]
A_left <- out_data[-which.max(out_data$F1),]
A <- A_left[which.max(A_left$F1),]
data_a <- read.csv(A$method, header=T,sep=",")
data_b <- read.csv(B$method, header=T,sep=",")
trues_a <- c()
trues_b <- c()
ab_test <- data.frame(A=data_a$prediction,B=data_b$prediction)
tab <- table(ab_test)
print(tab)
View(B)
B <- out_data[which.max(out_data$F1),]
A_left <- out_data[-which.max(out_data$F1),]
A <- A_left[which.max(A_left$F1),]
View(B)
View(A)
data_a <- read.csv(A$method, header=T,sep=",")
data_b <- read.csv(B$method, header=T,sep=",")
View(data_a)
trues_a <- c()
trues_b <- c()
mytable <- function(mymodel,data){
prob <-as.data.frame(predict(mymodel,data[,-2],type="prob"))
prob_res <-as.data.frame(predict(mymodel,data[,-2],type="response"))
# result <- ifelse(prob$prediction=="Yes",prob$Yes,prob$No)
prob$pred.score <- prob$Yes
prob$reference <- data[,2]
prob$prediction <- prob_res[,1]
d <- prob[,c(-1:-2)]
d
}
data_a <- mytable(mymodel,data_a)
View(data_a)
data_b <- mytable(mymodel,data_b)
trues_a <- c()
trues_b <- c()
ab_test <- data.frame(A=data_a$prediction,B=data_b$prediction)
index<-sapply(out_data[,c("sensitivity","specificity","F1","AUC")], max)
out_data <-data.frame("method"=files, "sensitivity"=sens, "specificity"=spes, "F1"=F1s, "AUC"=AUCs, stringsAsFactors = F)
index<-sapply(out_data[,c("sensitivity","specificity","F1","AUC")], max)
out_data<-rbind(out_data,c("highest",names[index]))
View(out_data)
query_func<-function(query_m, i)
{
if(query_m == "male"){
which.max(i)
}
else if (query_m == "female") {
which.max(i)
} else {
stop(paste("ERROR: unknown query function", query_m))
}
}
query_m = "male"
out_data <-data.frame("method"=files, "sensitivity"=sens, "specificity"=spes, "F1"=F1s, "AUC"=AUCs, stringsAsFactors = F)
index<-sapply(out_data[,c("sensitivity","specificity","F1","AUC")], query_func, query_m=query_m)
out_data<-rbind(out_data,c("highest",names[index]))
View(out_data)
out_data <-data.frame("method"=names, "sensitivity"=sens, "specificity"=spes, "F1"=F1s, "AUC"=AUCs, stringsAsFactors = F)
index<-sapply(out_data[,c("sensitivity","specificity","F1","AUC")], query_func, query_m=query_m)
out_data<-rbind(out_data,c("highest",names[index]))
View(out_data)
out_data['method'] = names
system("Rscript final_105753032.R -fold 5 -out performance.csv")
system("Rscript final_105753032.R -fold 5 -out performance.csv")
system("Rscript final_105753032.R -fold 5 -out performance.csv")
shiny::runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/ass4')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/ass4')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/ass4')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
system("Rscript final_105753032.R -fold 5 -out performance.csv")
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
library(plotly)
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
data <- out_data[,c(input$x,input$y)]
data <- out_data[,c("sensitivity","specificity")]
plot_ly(data = data , x = ~input$x, y = ~input$y )
plot_ly(data = data , x = "sensitivity", y = "specificity" )
plot_ly(data = data , x = "sensitivity", y = "specificity" ,color = ~carat,
size = ~carat, text = ~paste("Clarity: ", clarity))
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
library(plotly)
packageVersion('plotly')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
install.packages("plotly")
install.packages("plotly")
shiny::runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
install.packages(plotly)
install.packages("plotly")
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
runApp('~/Google 雲端硬碟/Graduate/1052/DataScience/1052DataScience/final_project')
