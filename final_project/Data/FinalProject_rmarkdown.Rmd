---
title: "R Final Project"
author: "YangMing"
date: "2017/6/20"
output: html_document
---

#You can follow this file to reproduce my result.
#Let's do it!

### 套件載入
```{r package, message=FALSE, warning=FALSE, include=FALSE}
library(plyr)
library(dplyr)
library(randomForest)
library(DT)
```



### Read dataset
```{r read dataset}
data <- read.csv("WA_Fn-UseC_-HR-Employee-Attrition.csv",header=T,na.strings='NULL')
datatable(data)

```


### Set Cross Validation
```{r cross validation, echo=TRUE, message=FALSE, warning=FALSE}
k = 10
set.seed(9487)
# sample from 1 to k, nrow times (the number of observations in the data)
data$id <- sample(1:k, nrow(data), replace = TRUE)
```

### Set model running Progress bar
```{r progress bar, echo=TRUE, message=FALSE, warning=FALSE}
progress.bar <- create_progress_bar("text")
progress.bar$init(k)
```

### Set Compute cal&test erro function
```{r function, echo=TRUE, message=FALSE, warning=FALSE}
myPred <- function(mymodel,dataset){
  prediction <- data.frame()
  testsetCopy <- data.frame()
  temp <- as.data.frame(predict(mymodel, dataset[,-2]))
  prediction <- rbind(prediction, temp)
  testsetCopy <- rbind(testsetCopy, as.data.frame(dataset[,2]))
  result <- cbind(prediction, testsetCopy[, 1])
  names(result) <- c("Predicted", "Actual")
  for (i in 1:length(result$Actual)){
    if (result$Actual[i] == result$Predicted[i]) {
      result$Difference[i] <- 1
    } else {
      result$Difference[i] <- 0
    }
  }
  
  predict_1 <- sum(result$Difference == 1)
  predict_all <- length(result$Difference)
  testErro <-predict_1 / predict_all
  
  testErro
}

trainErros <- c()
calErrors <- c()
testErrors <- c()

```

### Run randomForest model
```{r model}

for (i in 1:k){
  # remove rows with id i from dataframe to create training set
  # select rows with id i to create test set
  dTrainAll <- subset(data, id != i)
  testset <- subset(data, id == i )
 
  
  useForCal <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=1/ (k-1))>0 	# Note: 14 
  dCal <- subset(dTrainAll,useForCal)
  dTrain <- subset(dTrainAll,!useForCal)
  
   mymodel <- randomForest(Attrition~.,data=dTrain,ntree=100,importance=TRUE,proximity=TRUE)
  
  testErro <- myPred(mymodel,testset)
  calError <- myPred(mymodel,dCal)

  table.model <- mymodel$confusion
  trainErro <- sum(diag(table.model)/sum(table.model))

  trainErros <- c(trainErros,trainErro)
  calErrors <- c(calErrors,calError)
  testErrors <- c(testErrors,testErro)

  write.csv(dTrainAll,paste0("method", i ,".csv") , row.names = F, quote = F)
  
  progress.bar$step()
}



```
### Calculate accuracy
```{r accuracy}
set <- c("trainning","calibration","test")
accuracy <- c(round(mean(trainErros),digits = 2),
              round(mean(calErrors),digits = 2),
              round(mean(testErrors), digits = 2))
out_data_result <-data.frame("set"=set, "accuracy"=accuracy,stringsAsFactors = F)

datatable(out_data_result)

```

### Calculate Sensitivity Specificity F1 AUC
```{r eff, echo=TRUE, message=FALSE, warning=FALSE}
files <- c()

for (i in 1:k) {
  file <- paste0("method", i ,".csv")
  files <- c(files,file)
}

names<-c()
sens<-c()
spes<-c()
F1s<-c()
AUCs<-c()

for(file in files){
  name<-gsub(".csv", "", basename(file))
  data<-read.table(file, header=T,sep=",")
  
  prob <-as.data.frame(predict(mymodel,data[,-2],type="prob"))
  prob_res <-as.data.frame(predict(mymodel,data[,-2],type="response"))
  result <- ifelse(prob$prediction=="Yes",prob$Yes,prob$No)
  prob$pred.score <- prob$Yes
  prob$reference <- data[,2]
  prob$prediction <- prob_res[,1]
  d <- prob[,c(-1:-2)]
  
  write.csv(d,paste0("methods/",file) , row.names = F, quote = F)
  
  cM <- table(truth=d$reference, prediction=d$prediction)
  sensitivity <- round ( cM[2, 2] / (cM[2, 1] + cM[2, 2]), digits = 2)
  specificity <- round ( cM[1, 1] / (cM[1, 1] + cM[1, 2]), digits = 2)
  precision <- round ( cM[2, 2] / (cM[1, 2] + cM[2, 2]), digits = 2)
  F1 <- round ( 2 *  precision * sensitivity / (precision + sensitivity), digits = 2)
  # AUC evaluation
  library('ROCR')
  eval <- prediction(d$pred.score,d$reference)
  AUC <- round(attributes(performance(eval,'auc'))$y.values[[1]], digits = 2)

  sens<-c(sens, sensitivity)
  spes<-c(spes, specificity)
  F1s<-c(F1s, F1)
  AUCs<-c(AUCs, AUC)
  names<-c(names,name)
}

```

### Output Sensitivity Specificity F1 AUC result
```{r eff output}
query_func<-function(query_m, i)
{
  if(query_m == "male"){
    which.max(i)
  }
  else if (query_m == "female") {
    which.max(i)
  } else {
    stop(paste("ERROR: unknown query function", query_m))
  }
}
query_m = "male"

out_data <-data.frame("method"=names, "sensitivity"=sens, "specificity"=spes, "F1"=F1s, "AUC"=AUCs, stringsAsFactors = F)
index<-sapply(out_data[,c("sensitivity","specificity","F1","AUC")], query_func, query_m=query_m)
out_data<-rbind(out_data,c("highest",names[index]))

datatable(out_data)

```



